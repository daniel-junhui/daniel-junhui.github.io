---
title: "6.S081 随记"
date: 2021-07-15T15:51:07+08:00
draft: true
---

因为其实一直都没有好好读过 OSTEP，所以我在最近一周一口气把这本书读完了，但还是感觉不够，因为我知道要学会这玩意必须要 make hands dirty。于是准备快速的把 6.S081 过完，然后把 lab 做掉，再配合读 xv6 的代码。

## LEC 2
### Syscall system call

第一个 lab 没什么好记录的，只不过用 system call 小打小闹而已，从第二个 lab 开始才是真的对 kernel 做事情。

#### 遇到的一些疑问

- 英语太垃，看不懂这句话：

  > Add a `sys_trace()` function in `kernel/sysproc.c` that implements the new system call by remembering its argument in a new variable in the proc structure (see `kernel/proc.h`). The functions to retrieve system call arguments from user space are in `kernel/syscall.c`, and you can see examples of their use in `kernel/sysproc.c`.

龙鸣翻译：要加一个新的函数，这个函数通过在 `proc` 结构中增加的一个新的变量来实现系统调用，从用户空间拿到系统调用参数的函数在 `syscall.c` 中。确实没怎么看懂，那就先看看代码。

一个很关键的汇编文件：`usys.S`

```assembly
sleep:
 li a7, SYS_sleep
 ecall
 ret
.global uptime
uptime:
 li a7, SYS_uptime
 ecall
 ret
.global trace
trace:
 li a7, SYS_trace
 ecall
 ret
```

这个文件可以看到调用 system call 的一些指令，其中 `a7` 是 RISC-V 的一个寄存器，再看 `kernel/syscall.c` 当中的一个关键的函数：

```c
void
syscall(void)
{
  int num;
  struct proc *p = myproc();

  num = p->trapframe->a7;
  if(num > 0 && num < NELEM(syscalls) && syscalls[num]) {
    p->trapframe->a0 = syscalls[num]();
  } else {
    printf("%d %s: unknown sys call %d\n",
            p->pid, p->name, num);
    p->trapframe->a0 = -1;
  }
}
```

这个函数应该就是用户态和系统调用的接口，首先获取当前进程的 `a7` 寄存器，如果这个寄存器中的系统调用号存在（也就是 `num > 0 && num < NELEM(syscalls) && syscalls[num] `语句为真），那就调用它并返回（把 `a0` 寄存器设置为返回值），否则就返回 `-1` 。

递归学习：看不太懂下面这个函数指针数组到底是什么玩意：

```c
static uint64 (*syscalls[])(void) = {
[SYS_fork]    sys_fork,
[SYS_exit]    sys_exit,
[SYS_wait]    sys_wait,
[SYS_pipe]    sys_pipe,
[SYS_read]    sys_read,
[SYS_kill]    sys_kill,
[SYS_exec]    sys_exec,
[SYS_fstat]   sys_fstat,
[SYS_chdir]   sys_chdir,
[SYS_dup]     sys_dup,
[SYS_getpid]  sys_getpid,
[SYS_sbrk]    sys_sbrk,
[SYS_sleep]   sys_sleep,
[SYS_uptime]  sys_uptime,
[SYS_open]    sys_open,
[SYS_write]   sys_write,
[SYS_mknod]   sys_mknod,
[SYS_unlink]  sys_unlink,
[SYS_link]    sys_link,
[SYS_mkdir]   sys_mkdir,
[SYS_close]   sys_close,
};
```

其实就是看不懂 `uint64 (*syscalls[])(void)` 的意思，参考之前知乎上看到的一个回答，把这个东西翻译成英文：
an array of pointers to a function that returns uint64，也就是一个函数指针的数组，这个初始化的方式也确实给我整麻了，但无所谓看懂了就行，不纠结语法。

其实还有一个终极问题：当调用系统调用的时候，真正的流程到底是什么？很遗憾这可能需要用 gdb 目力调试才能知道，但其实在不知道这个事情的流程下也能够完成这个实验，只需要观察一下 `sys_exit` 的代码：

```c
uint64
sys_exit(void)
{
  int n;
  if(argint(0, &n) < 0)
    return -1;
  exit(n);
  return 0;  // not reached
}
```

它调用了 `argint(0, &n)` 来获取了进程退出时的状态号，再看到 `syscall.c` 中对于 `argint` 函数的注释：`// Fetch the nth 32-bit system call argument.` 就可以知道这个函数就是用来获取系统调用的参数的，那直接先拿来用就好了（拿来主义）。

最终增加的关键代码如下：（还有一些杂七杂八的东西要加，比如系统调用的打印）

```c
uint64
sys_trace(void)
{
  uint mask;
  if(argint(0, (int *)&mask) < 0)
    return -1;
  myproc()->trace_mask = mask;
  return 0;
}
```

### RISC-V 系统调用的一些规则摘录

- syscall number is passed in `a7`
- syscall arguments are passed in `a0` to `a5`
- unused arguments are set to `0`
- return value is returned in `a0`

### Sysinfo system call

差不多和前面的 system call 一样，需要把一些信息拷贝到 user 空间，要用到 `copyout()` 函数。

```c
// Copy from kernel to user.
// Copy len bytes from src to virtual address dstva in a given page table.
// Return 0 on success, -1 on error.
int
copyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len)
{
  uint64 n, va0, pa0;

  while(len > 0){
    va0 = PGROUNDDOWN(dstva);
    pa0 = walkaddr(pagetable, va0);
    if(pa0 == 0)
      return -1;
    n = PGSIZE - (dstva - va0);
    if(n > len)
      n = len;
    memmove((void *)(pa0 + (dstva - va0)), src, n);

    len -= n;
    src += n;
    dstva = va0 + PGSIZE;
  }
  return 0;
}
```

其实也很简单，直接拿到当前进程的 pagetable，然后虚拟地址其实就是第一个参数，其他的直接传进去用就好了。

至于活跃进程数量和空闲内存数：

- xv6 中的进程比较简单，是一个固定的数组（64个）。遍历一遍，剔除 `UNUSED` 的数量就好了；
- 先获取空闲的页数，然后乘以 4K 就可以得到空闲字节数。

```c
uint64
sys_sysinfo(void)
{
  // printf("hello\n");
  struct sysinfo info;
  uint64 user_addr;
  struct proc *p = myproc();
  if(argaddr(0, &user_addr) < 0)
    return -1;
  info.freemem = free_memory_number();
  info.nproc = process_number();
  if(copyout(p->pagetable, user_addr, (char*)&info, sizeof(struct sysinfo))<0)
    return -1;
  return 0;
}
```
## LEC 3
### 每个进程的 kernel page table
说实话，做到这一步的时候，我很莫名其妙为什么要去修改内核的这个机制，也就是「内核拥有一个自己的页表」机制。

读了一下 xv6 的 rec-book，这个 lab 的意思大致就是因为 xv6 的内核采用的是 direct-mapping 机制，所以可以让「内核拥有一个自己的页表」，但这样的话在运行内核代码的时候，就要用一个「转换机制」来访问用户的数据，接下来几个事情就是为了改变这个事情，至于改了之后到底有什么好处，我也不知道，做了再说。

大致的思路是这样：

- 首先是需要给每个 process 维护一个 kernel_pagetable 域。
- 需要给每个 process 创建时（也就是在函数 `allocproc` 中）填充这个 process 的 kernel_pagetable ，目前来说每个进程的内核页表是和内核独有的页表是一模一样的。
- 除了上面和内核独有页表的内容一样以外，每个进程都要在自己的页表里再增加自己的内核 stack 映射。而这个映射原本在未修改的 xv6 中是由内核提前为所有的预备进程做好的（ `procinit` 函数），现在需要把这个事情延迟到在每个进程被创建的时候再做。
- 在调度函数中确保在切换到某个进程的之前，使用该进程的内核页表（也就是在 `swtch(&c->context, &p->context)` 之前），在重新回到调度程序后，再切换回内核自己的页表。
- 在进程被 free 的时候，顺便把 kernel_pagetable 也删了，但不用删除页表的叶结点，只需要删前两层的索引。

在差不多做完这些事情后，还遇到了一些问题。

1. 改完之后 `make qemu` 直接报 `panic: kvmpa` 。

`kvmpa` 函数的用处是「将内核的一个虚拟地址转换为一个物理地址」，而 xv6 的内核地址大多都是 direct-mapping 的，换言之其实就是转换内核栈的地址而已，而在改完之后，进程的内核栈的地址已经不属于内核的页表了，而属于每个进程自己的内核页表，因此需要修改 `pte = walk(kernel_pagetable, va, 0)` 为 `pte = walk(myproc()->kernel_pagetable, va, 0)` 。（调用 `myproc()` 需要加两个头文件）

2. 进程内核栈的内存泄漏问题

要注意，每个进程被 free 的时候，即使不用释放进程内核页表绝大多数的叶子 page ，但是进程内核栈的 page 是必须要释放的，不然跑 usertests 的时候会在 sbrkfail 处报 kvmmap 的 panic ，我没有用 gdb 去调试出到底出了什么差错。但在我添加了对内核栈的手动释放后，就不报错了，所以很大可能的原因就是内存泄漏了，每当创建一个进程就浪费掉 1 个 page，free 的时候又不回收，最后就导致内存用尽没得用了。

```c
  if(p->kstack)
    uvmunmap(p->kernel_pagetable, p->kstack, 1, 1); 
  p->kstack = 0;
  if(p->kernel_pagetable)
    proc_freekernelpagetable(p->kernel_pagetable);
  p->kernel_pagetable = 0;
```

